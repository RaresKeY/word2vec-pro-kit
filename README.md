# Professional Word2Vec Pipeline: Scalable & Optimized
[![Hugging Face Model](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-orange)](https://huggingface.co/LogicLark-QuantumQuill/static-embeddings-en-50m-v1)

## ðŸŒŸ Quick Showcase: Pre-Trained 50M Model

If you have already trained the model (or downloaded `word2vec_sampled_50m.safetensors`), you can test it immediately. The system automatically handles both modern `.safetensors` and legacy `.pth` formats.

```bash
# Run automated benchmarks (default: word2vec_sampled_50m.safetensors)
python automated_tests.py

# Explore interactively
python test.py
```

### Actual Performance Examples:
The following are real results from our 50-million-word balanced training run:

*   **Logic (Analogies):**
    *   `King - Man + Woman` = **Queen** (Rank #1)
    *   `Paris - France + Germany` = **Berlin** (Rank #1)
    *   `Brother - Man + Woman` = **Sister** (Rank #1)
    *   `Write + Did - Do` = **Wrote** (Correct Grammar!)
*   **Intuition (Odd One Out):**
    *   `[apple, banana, orange, car]` â†’ **car**
    *   `[germany, france, italy, tokyo]` â†’ **tokyo**
*   **Clustering (Similarity):**
    *   `Computer` â†’ `laptop`, `mainframe`, `software`, `programmer`
    *   `Physics` â†’ `chemistry`, `astronomy`, `mathematics`

---

## ðŸš€ Quick Start (Training a New Model)

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Download Data
Download the "Golden Trio" (Wikipedia, Books, Web) with parallel streams:
```bash
# Limit to 500MB per dataset for a fast 2GB local run
python download_golden_trio.py --limit_per_dataset 500 --output_dir data/raw_parts
```

### 3. Balanced Sampling & Training
Run the optimized sampling trainer to create a high-quality 50M word model. By default, it saves in the secure `.safetensors` format.
```bash
python train_sampled.py --epochs 10 --save_path my_model.safetensors
```
*Note: This script automatically expects data in 'data/raw_parts' and saves a sampled corpus to 'data/sampled_50m.txt'. A companion `.vocab.pth` file will be created alongside `.safetensors` models.*


### 4. Evaluate
Run the automated benchmark suite:
```bash
python automated_tests.py my_model.safetensors
```

---

## ðŸ›  Advanced Features

### 1. Safetensors Integration
The pipeline uses **Hugging Face Safetensors** for model storage. 
*   **Safety:** Prevents arbitrary code execution (unlike pickle-based `.pth`).
*   **Speed:** Near-instant loading via memory-mapping (mmap).
*   **Metadata:** Vocabulary is stored in a companion `.vocab.pth` for transparent metadata management.

### 2. Vectorized FastBatcher
Traditional PyTorch loops are slow. This pipeline uses **Numpy C-accelerated slicing** to prepare batches, increasing throughput by up to 50x and ensuring your GPU is never idle.

### 3. Binary Pre-Tokenization
To eliminate CPU string-processing bottlenecks, the script converts raw text into a compact binary format (`.tokens.bin`) once. During training, it reads raw integer IDs directly from RAM or memory-mapped disk.

### 4. Balanced Sampling
Instead of training on a single biased dataset, `train_sampled.py` jumps through multiple sources (Wikipedia, Fiction, News) to build a representative "world-view" for the model.

---

## ðŸ“‚ Project Structure
- `train_sampled.py`: Main trainer with balanced sampling and vectorized batching. Saves `.safetensors` by default.
- `download_golden_trio.py`: Parallel downloader for high-quality datasets.
- `automated_tests.py`: Evaluation suite (Similarities, Analogies, Odd-One-Out). Supports both `.safetensors` and `.pth`.
- `test.py`: Interactive CLI tool for manual model exploration.
- `PROJECT_CONCEPTS.md`: Detailed explanation of the math and optimization logic.

---
*Created with Gemini CLI Agent*
